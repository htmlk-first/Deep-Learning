{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhoQ0WE77laV"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:09.802137Z",
     "iopub.status.busy": "2022-12-15T01:55:09.801662Z",
     "iopub.status.idle": "2022-12-15T01:55:09.806199Z",
     "shell.execute_reply": "2022-12-15T01:55:09.805547Z"
    },
    "id": "_ckMIh7O7s6D"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# 훈련 루프와 함께 tf.distribute.Strategy 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5Uhzt6vVIB2"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/distribute/custom_training\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> TensorFlow.org에서 보기</a>\n",
    "</td>\n",
    "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/distribute/custom_training.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/distribute/custom_training.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
    "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/distribute/custom_training.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbVhjPpzn6BM"
   },
   "source": [
    "이 튜토리얼에서는 `tf.distribute.Strategy` TensorFlow API와 사용자 정의 훈련 루프를 사용하여 여러 처리 장치(GPU, 여러 머신 또는 TPU)에 [훈련을 배포하기](../../guide/distributed_training.ipynb) 위한 추상화 방법을 보여줍니다. 이 예에서는 28 x 28 크기의 이미지 70,000개가 포함된 [Fashion MNIST 데이터세트](https://github.com/zalandoresearch/fashion-mnist)에서 간단한 컨볼루션 신경망을 훈련합니다.\n",
    "\n",
    "[사용자 정의 훈련 루프](../customization/custom_training_walkthrough.ipynb)는 훈련에 대한 유연성과 더 나은 통제력을 제공합니다. 또한 모델과 훈련 루프를 쉽게 디버그할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:09.810440Z",
     "iopub.status.busy": "2022-12-15T01:55:09.809899Z",
     "iopub.status.idle": "2022-12-15T01:55:12.040361Z",
     "shell.execute_reply": "2022-12-15T01:55:12.039438Z"
    },
    "id": "dzLKpmZICaWN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 01:55:10.915850: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 01:55:10.915956: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 01:55:10.915967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM6W__qraV55"
   },
   "source": [
    "## Fashion MNIST 데이터세트 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:12.045119Z",
     "iopub.status.busy": "2022-12-15T01:55:12.044230Z",
     "iopub.status.idle": "2022-12-15T01:55:12.559018Z",
     "shell.execute_reply": "2022-12-15T01:55:12.558038Z"
    },
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Add a dimension to the array -> new shape == (28, 28, 1)\n",
    "# This is done because the first layer in our model is a convolutional\n",
    "# layer and it requires a 4D input (batch_size, height, width, channels).\n",
    "# batch_size dimension will be added later on.\n",
    "train_images = train_images[..., None]\n",
    "test_images = test_images[..., None]\n",
    "\n",
    "# Scale the images to the [0, 1] range.\n",
    "train_images = train_images / np.float32(255)\n",
    "test_images = test_images / np.float32(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AXoHhrsbdF3"
   },
   "source": [
    "## 변수와 그래프를 분산하는 전략 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mVuLZhbem8d"
   },
   "source": [
    "`tf.distribute.MirroredStrategy` 전략이 어떻게 동작할까요?\n",
    "\n",
    "- 모든 변수와 모델 그래프는 복제본 간에 복제됩니다.\n",
    "- 입력은 장치에 고르게 분배되어 들어갑니다.\n",
    "- 각 장치는 주어지는 입력에 대해서 손실(loss)과 그래디언트를 계산합니다.\n",
    "- 그래디언트들을 전부 더함으로써 모든 장치들 간에 그래디언트들이 동기화됩니다.\n",
    "- 동기화된 후에, 동일한 업데이트가 각 장치에 있는 변수의 복사본(copies)에 동일하게 적용됩니다.\n",
    "\n",
    "참고: 아래의 모든 코드를 단일 범위에 넣을 수 있습니다. 이 예에서는 설명을 위해 여러 코드 셀로 나눕니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:12.563856Z",
     "iopub.status.busy": "2022-12-15T01:55:12.563129Z",
     "iopub.status.idle": "2022-12-15T01:55:17.442420Z",
     "shell.execute_reply": "2022-12-15T01:55:17.441357Z"
    },
    "id": "F2VeZUWUj5S4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "# If the list of devices is not specified in\n",
    "# `tf.distribute.MirroredStrategy` constructor, they will be auto-detected.\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:17.446518Z",
     "iopub.status.busy": "2022-12-15T01:55:17.445921Z",
     "iopub.status.idle": "2022-12-15T01:55:17.450546Z",
     "shell.execute_reply": "2022-12-15T01:55:17.449487Z"
    },
    "id": "ZngeM_2o0_JO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 4\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k53F5I_IiGyI"
   },
   "source": [
    "## 입력 파이프라인 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:17.454884Z",
     "iopub.status.busy": "2022-12-15T01:55:17.454265Z",
     "iopub.status.idle": "2022-12-15T01:55:17.458378Z",
     "shell.execute_reply": "2022-12-15T01:55:17.457484Z"
    },
    "id": "jwJtsCQhHK-E"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_images)\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7fj3GskHC8g"
   },
   "source": [
    "데이터세트를 만들고 배포합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:17.462319Z",
     "iopub.status.busy": "2022-12-15T01:55:17.461716Z",
     "iopub.status.idle": "2022-12-15T01:55:18.222553Z",
     "shell.execute_reply": "2022-12-15T01:55:18.221510Z"
    },
    "id": "WYrMNNDhAvVl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 01:55:18.156270: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 60000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-12-15 01:55:18.216329: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:3\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE) \n",
    "\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAXAo_wWbWSb"
   },
   "source": [
    "## 모델 만들기\n",
    "\n",
    "`tf.keras.Sequential`을 사용하여 모델을 만듭니다. [모델 하위 클래스화 API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) 또는 [함수형 API](https://www.tensorflow.org/guide/keras/functional)를 사용하여 이를 수행할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:18.227358Z",
     "iopub.status.busy": "2022-12-15T01:55:18.226509Z",
     "iopub.status.idle": "2022-12-15T01:55:18.232211Z",
     "shell.execute_reply": "2022-12-15T01:55:18.231325Z"
    },
    "id": "9ODch-OFCaW4"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:18.236209Z",
     "iopub.status.busy": "2022-12-15T01:55:18.235631Z",
     "iopub.status.idle": "2022-12-15T01:55:18.239696Z",
     "shell.execute_reply": "2022-12-15T01:55:18.238784Z"
    },
    "id": "9iagoTBfijUz"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-VVTqDEICrl"
   },
   "source": [
    "## 손실 함수 정의하기\n",
    "\n",
    "일반적으로 단일 GPU/CPU가 있는 단일 시스템에서 손실 함수는 입력 배치의 예제 수로 나뉩니다.\n",
    "\n",
    "*그렇다면 `tf.distribute.Strategy`를 사용할 때 손실을 어떻게 계산해야 할까요?*\n",
    "\n",
    "- 예를 들어 GPU가 4개 있고 배치 크기가 64라고 가정해 보겠습니다. 하나의 입력 배치가 전체 복제본(4개의 GPU)에 걸쳐 분배되고 각 복제본은 크기 16의 입력을 받습니다.\n",
    "\n",
    "- 각 복제본의 모델은 해당 입력으로 순방향 전달을 수행하고 손실을 계산합니다. 이제 손실을 해당 입력의 예제 수로 나누는 대신(BATCH_SIZE_PER_REPLICA = 16), 손실을 GLOBAL_BATCH_SIZE(64)로 나누어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCIcsaeoIHJX"
   },
   "source": [
    "*이렇게 하는 이유는 무엇일까요?*\n",
    "\n",
    "- 각 복제본에서 그래디언트가 계산된 후 이를 **합산**하여 전체 복제본에 걸쳐 동기화되기 때문에 이렇게 해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-wlFFZbP33n"
   },
   "source": [
    "*TensorFlow에서 이 작업을 어떻게 수행할까요?*\n",
    "\n",
    "- 이 튜토리얼에서와 같이 사용자 지정 훈련 루프를 작성하는 경우 예제당 손실을 합산하고 합계를 GLOBAL_BATCH_SIZE로 나누어야 합니다: `scale_loss = tf.reduce_sum(loss) * (1. / GLOBAL_BATCH_SIZE)` 또는 `tf.nn.compute_average_loss`를 사용하여 예제당 손실, 선택적 샘플 가중치, GLOBAL_BATCH_SIZE를 인수로 사용하고 조정된 손실을 반환할 수 있습니다.\n",
    "\n",
    "- 모델에서 정규화 손실을 사용하는 경우 복제본 수에 따라 손실 값을 확장해야 합니다. `tf.nn.scale_regularization_loss` 함수를 사용하여 이를 수행할 수 있습니다.\n",
    "\n",
    "- `tf.reduce_mean`을 사용하는 것은 권장하지 않습니다. 이렇게 하면 손실이 실제 복제본 배치 크기별로 나눠지는데, 이는 단계별로 다를 수 있습니다.\n",
    "\n",
    "- 이 축소 및 크기 조정은 keras `model.compile` 및 `model.fit`에서 자동으로 수행됩니다.\n",
    "\n",
    "- `tf.keras.losses` 클래스를 사용하는 경우(아래 예와 같이) 손실 축소는 `NONE` 또는 `SUM` 중 하나로 명시적으로 지정되어야 합니다. `AUTO` 및 `SUM_OVER_BATCH_SIZE`는 `tf.distribute.Strategy`와 함께 사용할 때 허용되지 않습니다. `AUTO`는 사용자가 어떤 축소가 필요한지 명시적으로 생각해야 하므로(분산된 경우에 이러한 축소가 올바른지 확인하기 위해) 허용되지 않습니다. `SUM_OVER_BATCH_SIZE`는 현재 복제본 배치 크기별로만 나누고 복제본 수로 나누는 일은 사용자에게 맡기는 데, 이를 쉽게 놓칠 수 있다는 점 때문에 허용되지 않습니다. 따라서 사용자가 직접 명시적으로 축소를 수행해야 합니다.\n",
    "\n",
    "- `labels`이 다차원인 경우 각 샘플의 요소 수에 걸쳐 `per_example_loss`의 평균을 구합니다. 예를 들어 `predictions`의 형상이 `(batch_size, H, W, n_classes)`이고 `labels`이 `(batch_size, H, W)`인 경우, 다음과 같이 `per_example_loss`를 업데이트해야 합니다: `per_example_loss /= tf.cast(tf.reduce_prod(tf.shape(labels)[1:]), tf.float32)`\n",
    "\n",
    "    주의: **손실의 형상을 확인하세요**. `tf.losses`/`tf.keras.losses`의 손실 함수는 일반적으로 입력의 마지막 차원에 대한 평균을 반환합니다. 손실 클래스는 이러한 함수를 래핑합니다. 손실 클래스의 인스턴스를 생성할 때 `reduction=Reduction.NONE`을 전달하는 것은 \"**추가적인** 축소가 없음\"을 의미합니다. `[batch, W, H, n_classes]`의 예제 입력 형상을 갖는 범주형 손실의 경우, `n_classes` 차원이 축소됩니다. `losses.mean_squared_error` 또는 `losses.binary_crossentropy`와 같은 포인트별 손실의 경우, 더미 축을 포함시켜 `[batch, W, H, 1]`이 `[batch, W, H]`로 축소되도록 합니다. 더미 축이 없으면 `[batch, W, H]`가 `[batch, W]`로 잘못 축소됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:18.243939Z",
     "iopub.status.busy": "2022-12-15T01:55:18.243393Z",
     "iopub.status.idle": "2022-12-15T01:55:18.248561Z",
     "shell.execute_reply": "2022-12-15T01:55:18.247638Z"
    },
    "id": "R144Wci782ix"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # Set reduction to `NONE` so you can do the reduction afterwards and divide by\n",
    "  # global batch size.\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True,\n",
    "      reduction=tf.keras.losses.Reduction.NONE)\n",
    "  def compute_loss(labels, predictions):\n",
    "    per_example_loss = loss_object(labels, predictions)\n",
    "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8y54-o9T2Ni"
   },
   "source": [
    "## 손실과 정확도를 기록하기 위한 지표 정의하기\n",
    "\n",
    "이 지표(metrics)는 테스트 손실과 훈련 정확도, 테스트 정확도를 기록합니다. `.result()`를 사용해서 누적된 통계값들을 언제나 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:18.252816Z",
     "iopub.status.busy": "2022-12-15T01:55:18.252085Z",
     "iopub.status.idle": "2022-12-15T01:55:18.327497Z",
     "shell.execute_reply": "2022-12-15T01:55:18.326774Z"
    },
    "id": "zt3AHb46Tr3w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='train_accuracy')\n",
    "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuKuNXPORfqJ"
   },
   "source": [
    "## 훈련 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:18.331726Z",
     "iopub.status.busy": "2022-12-15T01:55:18.331155Z",
     "iopub.status.idle": "2022-12-15T01:55:18.372663Z",
     "shell.execute_reply": "2022-12-15T01:55:18.371635Z"
    },
    "id": "OrMmakq5EqeQ"
   },
   "outputs": [],
   "source": [
    "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "  model = create_model()\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:18.377168Z",
     "iopub.status.busy": "2022-12-15T01:55:18.376549Z",
     "iopub.status.idle": "2022-12-15T01:55:18.382593Z",
     "shell.execute_reply": "2022-12-15T01:55:18.381557Z"
    },
    "id": "3UX43wUu04EL"
   },
   "outputs": [],
   "source": [
    "def train_step(inputs):\n",
    "  images, labels = inputs\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images, training=True)\n",
    "    loss = compute_loss(labels, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_accuracy.update_state(labels, predictions)\n",
    "  return loss \n",
    "\n",
    "def test_step(inputs):\n",
    "  images, labels = inputs\n",
    "\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss.update_state(t_loss)\n",
    "  test_accuracy.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:55:18.386739Z",
     "iopub.status.busy": "2022-12-15T01:55:18.386037Z",
     "iopub.status.idle": "2022-12-15T01:56:11.269736Z",
     "shell.execute_reply": "2022-12-15T01:56:11.268912Z"
    },
    "id": "gX975dMSNw0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6803293824195862, Accuracy: 75.58499908447266, Test Loss: 0.47986483573913574, Test Accuracy: 82.55000305175781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.41395682096481323, Accuracy: 85.15166473388672, Test Loss: 0.4264320433139801, Test Accuracy: 84.72999572753906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.3555366098880768, Accuracy: 87.24166870117188, Test Loss: 0.3662477135658264, Test Accuracy: 86.47999572753906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.3242177665233612, Accuracy: 88.31499481201172, Test Loss: 0.34345629811286926, Test Accuracy: 88.02000427246094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.30220723152160645, Accuracy: 89.08499908447266, Test Loss: 0.33098432421684265, Test Accuracy: 87.98999786376953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.28694629669189453, Accuracy: 89.50666809082031, Test Loss: 0.31467700004577637, Test Accuracy: 88.5999984741211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.2713523507118225, Accuracy: 90.05332946777344, Test Loss: 0.3279726803302765, Test Accuracy: 88.04000091552734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.2599695324897766, Accuracy: 90.46833801269531, Test Loss: 0.2958003878593445, Test Accuracy: 89.38999938964844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.2476484477519989, Accuracy: 90.95999908447266, Test Loss: 0.3058784306049347, Test Accuracy: 88.55999755859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.23547829687595367, Accuracy: 91.32500457763672, Test Loss: 0.284298300743103, Test Accuracy: 89.59000396728516\n"
     ]
    }
   ],
   "source": [
    "# `run` replicates the provided computation and runs it\n",
    "# with the distributed input.\n",
    "@tf.function\n",
    "def distributed_train_step(dataset_inputs):\n",
    "  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
    "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(dataset_inputs):\n",
    "  return strategy.run(test_step, args=(dataset_inputs,))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # TRAIN LOOP\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  for x in train_dist_dataset:\n",
    "    total_loss += distributed_train_step(x)\n",
    "    num_batches += 1\n",
    "  train_loss = total_loss / num_batches\n",
    "\n",
    "  # TEST LOOP\n",
    "  for x in test_dist_dataset:\n",
    "    distributed_test_step(x)\n",
    "\n",
    "  if epoch % 2 == 0:\n",
    "    checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "              \"Test Accuracy: {}\")\n",
    "  print(template.format(epoch + 1, train_loss,\n",
    "                         train_accuracy.result() * 100, test_loss.result(),\n",
    "                         test_accuracy.result() * 100))\n",
    "\n",
    "  test_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1YvXqOpwy08"
   },
   "source": [
    "위의 예제에서 주목해야 하는 부분\n",
    "\n",
    "- `for x in ...` 구문을 사용하여 `train_dist_dataset` 및 `test_dist_dataset`를 반복합니다.\n",
    "- 스케일이 조정된 손실은 `distributed_train_step`의 반환값입니다. `tf.distribute.Strategy.reduce` 호출을 사용해서 장치들 간의 스케일이 조정된 손실 값을 전부 합칩니다. 그리고 나서 `tf.distribute.Strategy.reduce` 반환 값을 더하는 식으로 배치 간의 손실을 모읍니다.\n",
    "- `tf.keras.Metrics`는 `tf.distribute.Strategy.run`에 의해 실행되는 `train_step` 및 `test_step` 내부에서 업데이트되어야 합니다.\n",
    "- `tf.distribute.Strategy.run`은 전략의 각 로컬 복제본에서 결과를 반환하며 이 결과를 사용하는 방법에는 여러 가지가 있습니다. `tf.distribute.Strategy.reduce`를 수행하여 집계된 값을 얻을 수 있습니다. `tf.distribute.Strategy.experimental_local_results`를 수행하여 로컬 복제본당 하나씩 결과에 포함된 값 목록을 가져올 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q5qp31IQD8t"
   },
   "source": [
    "## 최신 체크포인트를 불러와서 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNW2P00bkMGJ"
   },
   "source": [
    "`tf.distribute.Strategy`를 사용해서 체크포인트가 만들어진 모델은 전략 사용 여부에 상관없이 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:56:11.273987Z",
     "iopub.status.busy": "2022-12-15T01:56:11.273278Z",
     "iopub.status.idle": "2022-12-15T01:56:11.312302Z",
     "shell.execute_reply": "2022-12-15T01:56:11.311532Z"
    },
    "id": "pg3B-Cw_cn3a"
   },
   "outputs": [],
   "source": [
    "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='eval_accuracy')\n",
    "\n",
    "new_model = create_model()\n",
    "new_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:56:11.316360Z",
     "iopub.status.busy": "2022-12-15T01:56:11.316076Z",
     "iopub.status.idle": "2022-12-15T01:56:11.320086Z",
     "shell.execute_reply": "2022-12-15T01:56:11.319410Z"
    },
    "id": "7qYii7KUYiSM"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_step(images, labels):\n",
    "  predictions = new_model(images, training=False)\n",
    "  eval_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:56:11.323575Z",
     "iopub.status.busy": "2022-12-15T01:56:11.323069Z",
     "iopub.status.idle": "2022-12-15T01:56:11.714217Z",
     "shell.execute_reply": "2022-12-15T01:56:11.713222Z"
    },
    "id": "LeZ6eeWRoUNq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after restoring the saved model without strategy: 88.55999755859375\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "  eval_step(images, labels)\n",
    "\n",
    "print('Accuracy after restoring the saved model without strategy: {}'.format(\n",
    "    eval_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbcI87EEzhzg"
   },
   "source": [
    "## 데이터셋에 대해 반복작업을 하는 다른 방법들\n",
    "\n",
    "### 반복자(iterator)를 사용하기\n",
    "\n",
    "만약 주어진 스텝의 수에 따라서 반복하기 원하면서 전체 데이터세트를 보는 것을 원치 않는다면, `iter`를 호출하여 반복자를 만들 수 있습니다. 그 다음 명시적으로 `next`를 호출합니다. 또한, tf.function 내부 또는 외부에서 데이터세트를 반복하도록 설정할 수 있습니다. 다음은 반복자를 사용하여 tf.function 외부에서 데이터세트를 반복하는 코드 예제입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:56:11.718479Z",
     "iopub.status.busy": "2022-12-15T01:56:11.717790Z",
     "iopub.status.idle": "2022-12-15T01:56:18.314986Z",
     "shell.execute_reply": "2022-12-15T01:56:18.314140Z"
    },
    "id": "7c73wGC00CzN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.24009506404399872, Accuracy: 90.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.22674480080604553, Accuracy: 91.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.22516441345214844, Accuracy: 91.7578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.24468576908111572, Accuracy: 90.546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.22958669066429138, Accuracy: 91.8359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.2196839302778244, Accuracy: 92.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.2384246289730072, Accuracy: 90.9765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.21778538823127747, Accuracy: 91.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.20954498648643494, Accuracy: 91.9921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.18693020939826965, Accuracy: 93.0859375\n"
     ]
    }
   ],
   "source": [
    "for _ in range(EPOCHS):\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  train_iter = iter(train_dist_dataset)\n",
    "\n",
    "  for _ in range(10):\n",
    "    total_loss += distributed_train_step(next(train_iter))\n",
    "    num_batches += 1\n",
    "  average_train_loss = total_loss / num_batches\n",
    "\n",
    "  template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
    "  print(template.format(epoch + 1, average_train_loss, train_accuracy.result() * 100))\n",
    "  train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxVp48Oy0m6y"
   },
   "source": [
    "### tf.function 내부에서 반복하기\n",
    "\n",
    "`for x in ...` 구조를 사용하거나 위에서 했던 것처럼 반복자를 생성하여 `tf.function` 내부의 전체 입력 `train_dist_dataset`에 대해 반복할 수도 있습니다. 아래 예제는 `@tf.function` 데코레이터로 훈련한 하나의 epoch를 래핑하고 함수 내에서 `train_dist_dataset`을 반복하는 것을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:56:18.318852Z",
     "iopub.status.busy": "2022-12-15T01:56:18.318237Z",
     "iopub.status.idle": "2022-12-15T01:56:32.506766Z",
     "shell.execute_reply": "2022-12-15T01:56:32.505841Z"
    },
    "id": "-REzmcXv00qm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:461: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.22137457132339478, Accuracy: 91.8316650390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.21201039850711823, Accuracy: 92.2366714477539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.20279081165790558, Accuracy: 92.58499908447266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.1953163594007492, Accuracy: 92.71666717529297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.18428874015808105, Accuracy: 93.19999694824219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.17805488407611847, Accuracy: 93.32833099365234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.17244744300842285, Accuracy: 93.54000091552734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.16432566940784454, Accuracy: 93.961669921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.1565566509962082, Accuracy: 94.34833526611328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.15320821106433868, Accuracy: 94.36499786376953\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def distributed_train_epoch(dataset):\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  for x in dataset:\n",
    "    per_replica_losses = strategy.run(train_step, args=(x,))\n",
    "    total_loss += strategy.reduce(\n",
    "      tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "    num_batches += 1\n",
    "  return total_loss / tf.cast(num_batches, dtype=tf.float32)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_loss = distributed_train_epoch(train_dist_dataset)\n",
    "\n",
    "  template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
    "  print(template.format(epoch + 1, train_loss, train_accuracy.result() * 100))\n",
    "\n",
    "  train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuZGXiyC7ABR"
   },
   "source": [
    "### 장치 간의 훈련 손실 기록하기\n",
    "\n",
    "노트: 일반적인 규칙으로, `tf.keras.Metrics`를 사용하여 샘플당 손실 값을 기록하고 장치 내부에서 값이 합쳐지는 것을 피해야 합니다.\n",
    "\n",
    "손실 크기 조정 계산이 수행되기 때문에 `tf.keras.metrics.Mean`을 사용하여 여러 복제본에서 훈련 손실을 추적하는 것은 권장되지 않습니다.\n",
    "\n",
    "예를 들어, 다음과 같은 조건의 훈련을 수행한다고 합시다.\n",
    "\n",
    "- 두개의 장치\n",
    "- 두개의 샘플들이 각 장치에 의해 처리됩니다.\n",
    "- 손실 값을 산출합니다: 각각의 장치에 대해 [2, 3]과 [4, 5]\n",
    "- Global batch size = 4\n",
    "\n",
    "손실의 스케일 조정을 하면, 손실 값을 더하고 전역 배치 크기로 나누어 각 장치에 대한 샘플당 손실값을 계산할 수 있습니다. 이 경우에는 `(2 + 3) / 4 = 1.25` 및 `(4 + 5) / 4 = 2.25`입니다.\n",
    "\n",
    "만약 `tf.keras.metrics.Mean`을 사용하여 두 복제본의 손실을 추적한다면 결과가 다릅니다. 이 예에서는 결과적으로 메트릭에서 `result()`가 호출될 때 `total` 3.50개와 `count` 2개가 되고, `total`/`count` = 1.75가 됩니다. `tf.keras.Metrics`로 계산된 손실은 동기화된 복제본 수에 해당하는 추가 인자로 조정됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xisYJaV9KZTN"
   },
   "source": [
    "### 예제와 튜토리얼\n",
    "\n",
    "사용자 정의 훈련루프를 포함한 분산 전략을 사용하는 몇 가지 예제가 있습니다.\n",
    "\n",
    "1. [분산 교육 가이드](../../guide/distributed_training)\n",
    "2. `MirroredStrategy`를 사용하는 [DenseNet](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/densenet/distributed_train.py) 예제.\n",
    "3. <code>MirroredStrategy</code> 및 `TPUStrategy`를 사용해서 훈련하는 <a>BERT</a> 예제. 이 예제는 분산 훈련 중에 어떻게 체크포인트로부터 불러오는지와 어떻게 주기적으로 체크포인트를 생성해 내는 지를 이해하기에 정말 좋습니다.\n",
    "4. `keras_use_ctl flag`를 사용해서 활성화 할 수 있는 MirroredStrategy를 이용해서 훈련되는 [NCF](https://github.com/tensorflow/models/blob/master/official/recommendation/ncf_keras_main.py) 예제\n",
    "5. `MirroredStrategy`을 사용해서 훈련되는 [NMT](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/nmt_with_attention/distributed_train.py) 예제.\n",
    "\n",
    "<a>배포 전략 가이드</a>의 <em>예제 및 튜토리얼</em>에서 더 많은 예제를 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hEJNsokjOKs"
   },
   "source": [
    "## 다음 단계\n",
    "\n",
    "- 모델에서 새로운 `tf.distribute.Strategy` API를 사용해 보세요.\n",
    "- TensorFlow 모델의 성능을 최적화하는 도구에 대해 자세히 알아보려면 [`tf.function`을 이용한 성능 향상](../../guide/function.ipynb) 및 [TensorFlow 프로파일러](../../guide/profiler.md) 가이드를 참조하세요.\n",
    "- 사용 가능한 배포 전략에 대한 개요를 제공하는 [TensorFlow의 분산 훈련](../../guide/distributed_training.ipynb) 가이드를 확인하세요."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_training.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
